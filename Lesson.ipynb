{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b509e99c",
      "metadata": {
        "id": "b509e99c"
      },
      "source": [
        "## Scipy and Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d84e61",
      "metadata": {
        "id": "81d84e61"
      },
      "source": [
        "In the last notebook we took a tour of the `numpy` and `scipy` stack as it relates to math and statistics. This notebook is going to focus on another subset of `scipy` functionality that is absolutely critical to data analytics. Nearly every statistical algorithm aside from Ordinary Least Squares (OLS) regression cannot be solved algebraically. Because they cannot be solved algebraically, they must be solved **numerically**.\n",
        "\n",
        "### Numeric Optimization\n",
        "\n",
        "Let's start with an example of optimization. Let's say that you know the demand function for tickets to watch a local sports franchise. You can write the inverse demand function as\n",
        "\n",
        "$$ P = 300 - \\frac{1}{2}Q $$\n",
        "\n",
        "and the total cost function as\n",
        "\n",
        "$$ TC = 4000 + 45Q $$\n",
        "\n",
        "In order to choose the right number of tickets to sell, you need to calculate the quantity of tickets that will maximize profits. We can calculate total revenue as $ TR = P \\times Q $, and we can calculate profit as $ \\Pi = TR - TC $. This means that our profit function is\n",
        "\n",
        "$$ \\Pi = 300Q - \\frac{1}{2}Q^2 - 4000 - 45Q $$\n",
        "\n",
        "In order to find the $Q$ associated with the highest achievable level of profit, we can use calculus to find the point at which the rate of change in the profit function is zero ($\\frac{\\partial\\Pi}{\\partial Q}=0$).\n",
        "\n",
        "$$ \\frac{\\partial\\Pi}{\\partial Q} = 300 - Q - 45 = 0 \\implies Q = 255$$\n",
        "\n",
        "So we can **algebraically** solve this particular problem. This isn't always the case. Using `scipy`, we can solve this same problem, as well as many algebraically intractable problems that might be more interesting to us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d805e4",
      "metadata": {
        "id": "c6d805e4"
      },
      "source": [
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/paraboloid.png\" width=\"200\" height=\"200\" />\n",
        "\n",
        "\n",
        "In any optimization problem, we need to find a way to get ourselves to the minimum, and to know when we get there. When we look at the above image, we are able to visually trace the functional shape (looks like a rainbow ice cream cone to me...) and locate the bottom of the function. What we want to do is utilize an algorithm to \"trace\" our way from an arbitrary starting point within a function to the optimal point in that function.\n",
        "\n",
        "In three or fewer dimensions, this is easy. Regressions and statistical models often live in worlds with 100's or 1000's (even millions sometimes) of dimensions. We can't visualize our way to the bottom of those functions!\n",
        "\n",
        "The class of algorithm that is used to solve these problems is called **gradient descent**.\n",
        "\n",
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/gradDesc.png\" width=\"400\" />\n",
        "\n",
        "**Gradient descent** is an algorithm that explores the shape of the function, and determines which direction is most likely to lead to the optimal point. Let's focus on minimization. We want to find our way to the *bottom* of a function, and we can use gradient descent to try to get there. Given any starting point, our goal is to check the direction in which we can move downward most quickly, and start moving in that direction. At some distance from our starting point, we will stop and re-evaluate the direction in which we would like to travel. Are we still heading downhill in the steepest direction? If we aren't, then we need to update our behavior.\n",
        "\n",
        "Our gradient descent algorithm will keep \"looking around\" and moving down until it reaches a point at which it can no longer move \"down\" in any meaningful way. That is the stopping point, and is treated as the optimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af96154",
      "metadata": {
        "id": "0af96154"
      },
      "source": [
        "With an intuitive understanding of how optimization will happen computationally, it's time to learn a bit more about the math and the code that will help us to achieve computational optimization.\n",
        "\n",
        "Consider a function, $f$, with two variables $x$ and $y$. Because there are two input variables in the function, it has two partial derivatives:\n",
        "\n",
        "$$ \\frac{\\partial f}{\\partial x} \\text{ and } \\frac{\\partial f}{\\partial y} $$\n",
        "\n",
        "Each partial derivative tells us how $f$ changes as we move in a particular dimension **all else equal**. The **gradient**, then, is the vector of all partial derivatives of a given function at any point along the function:\n",
        "\n",
        "$$ \\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x} \\\\ \\\\ \\frac{\\partial f}{\\partial y} \\end{matrix} \\right]  $$\n",
        "\n",
        "We can use the gradient to determine the linear approximation of a function at any given point. Think about the gradient as the mathematical representation of the slope and direction of a hill you are hiking on. If we know the gradient, we know which way is down. As we continue to calculate gradients while walking, we can continue to ensure that we will walk downhill until we reach the bottom of the hill.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3fefac",
      "metadata": {
        "id": "6b3fefac"
      },
      "source": [
        "The steps of our gradient descent function will be the following:\n",
        "\n",
        "- Evaluate the gradient of the function\n",
        "- Find the direction of steepest descent\n",
        "- Determine how far to move in that direction\n",
        "- Move to new point\n",
        "- Reevaluate the gradient\n",
        "- Stop moving when gradient is within a margin of error from 0\n",
        "\n",
        "Let's try to implement gradient descent by solving our old profit maximization problem computationally. The very first thing that we need to do is write a Python function that will represent our mathematical function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c641a5e",
      "metadata": {
        "id": "3c641a5e"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    return tr - tc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1c0514",
      "metadata": {
        "id": "ed1c0514"
      },
      "source": [
        "This function will allow us to calculate profit at any output level based on our assumed total costs and demand curve. With this function, we can quickly calculate the gradient (in this case, just a simple derivative because our function is univariate) by calculating profit at two nearby points, and dividing by the horizontal distance between those points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef8c0ac",
      "metadata": {
        "id": "4ef8c0ac"
      },
      "outputs": [],
      "source": [
        "# Gradient at q=200\n",
        "\n",
        "(profit(201) - profit(199))/2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e219ac5",
      "metadata": {
        "id": "5e219ac5"
      },
      "source": [
        "    55.0\n",
        "\n",
        "\n",
        "\n",
        "Thus, a one unit increase in output at $Q=200$ results in a $55 increase in profits. This is cool, but it isn't enough for us to find the point of maximum profit (the optimal point). For that, we will need to calculate LOTS of gradients in order to move along the function until we cannot increase profits any further.\n",
        "\n",
        "Fortunately for us, `scipy` comes with optimization tools that will do all of the heavy lifting of the \"search\" for the optimal point. All that we need to do is frame our question algorithmically, and let `scipy` do the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edaa6713",
      "metadata": {
        "id": "edaa6713"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4f0c01",
      "metadata": {
        "id": "6b4f0c01"
      },
      "source": [
        "We start by importing the `minimize` function from `scipy.optimize`. Hang on! Weren't we working on a MAXIMIZATION problem?? What are we doing here?\n",
        "\n",
        "Maximization and minimization are the **same thing**. To maximize a function, you can multiply that function by `-1` and then calculate the minimum of the new \"upside-down\" function. It is functionally equivalent. So, in computational optimization, we always minimize.\n",
        "\n",
        "### Prepping for optimization\n",
        "\n",
        "As we prepare to optimize, there are two common problems with our function that we may need to resolve:\n",
        "\n",
        "1) When using `minimize` we can only pass an array of inputs, so we have to be careful to write our function accordingly\n",
        "2) Our problem is concave, and so has a maximum\n",
        "\t- We need to restate it as a minimization problem\n",
        "\n",
        "Problem 1 does not apply, since our function in univariate. In order to make our problem a minimization problem, we can flip our profit maximization function. We will simply return -1 * profit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c950291a",
      "metadata": {
        "id": "c950291a"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    pi =  tr - tc # didn't name it profit because that is our function's name! Don't want to clutter our name space!\n",
        "    return -1*pi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b1a329",
      "metadata": {
        "id": "56b1a329"
      },
      "source": [
        "### Making the call to `minimize`\n",
        "\n",
        "Now that our function is ready, it is time to minimize! The `minimize` function takes two arguments:\n",
        "1. Our function that we want to optimize\n",
        "2. A starting guess (as a vector)\n",
        "\n",
        "Let's give it a try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d46c4fa",
      "metadata": {
        "id": "7d46c4fa"
      },
      "outputs": [],
      "source": [
        "res = minimize(profit, [0]) # provide function and starting inputs\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61532439",
      "metadata": {
        "id": "61532439"
      },
      "source": [
        "          fun: -28512.499999980355\n",
        "     hess_inv: array([[1.00000175]])\n",
        "          jac: array([0.])\n",
        "      message: 'Optimization terminated successfully.'\n",
        "         nfev: 21\n",
        "          nit: 3\n",
        "         njev: 7\n",
        "       status: 0\n",
        "      success: True\n",
        "            x: array([255.00019821])\n",
        "\n",
        "\n",
        "\n",
        "That's it! No calculus, no searching, no checking gradients manually. `minimize` simply takes our function and our starting guess and brings us back the optimal choice. We get lots of information stored in the attributes of the `res` object:\n",
        "\n",
        "- `fun` provides the value of the function (this is -1 times the profit level at the optimal output in our example)\n",
        "- `hess_inv` and `jac` are measures of gradient and are used to determine how far to go and in which direction\n",
        "- `message` should be self-explanatory\n",
        "- `nfev` is the number of times the function (in this case `profit`) was evaluated during the search\n",
        "- `nit` is the number of iterations it took to find the optimum\n",
        "- `njev` is the number of times the Jacobian was estimated\n",
        "- `status` is a code associated with the `message` and `success` atrributes\n",
        "- `success` tells you whether or not an optimum was found (sometimes it cannot be easily found!)\n",
        "- `x` probably the most important attribute. This tells us the optimal input value (in our case $Q$), or set of values depending on our function. In a regression context, this could represent the fitted coefficients!\n",
        "\n",
        "Going forward, you will realize (especially because so many of them print the output as they optimize) just how many libraries in Python use this minimize function behind the scenes. It is used in `statsmodels`, `sklearn`, and many other high-profile libraries! Now that you know where it really happens (in `scipy`!), you'll be better able to troubleshoot the problems that will inevitably arise as you use statistical models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4035bfcd",
      "metadata": {
        "id": "4035bfcd"
      },
      "source": [
        "**Solve-it!**\n",
        "\n",
        "In this lesson we learned about optimization using SciPy. For the assignment this week, I would like you to build off of your `RegressionModel` class. You will add a Logistic Regression (Logit) method to your class, so that when the `regression_type` parameter is `logit`, Logistic Regression Results are returned.\n",
        "\n",
        "Your job is to create the following functionality within your class object:\n",
        "- a method (call it `logistic_regression`) that estimates the results of logistic regression using your `x` and `y` data frames, and using a likelihood function and gradient descent (DO NOT USE PREBUILT REGRESSION FUNCTIONS).\n",
        "    - You need to write a function to calculate the Log-likelihood of your model\n",
        "    - You need to implement gradient descent to find the optimal values of beta\n",
        "    - You need to use your beta estimates, the model variance, and calculate the standard errors of the coefficients, as well as Z statistics and p-values\n",
        "    - the results should be stored in a dictionary named `results`, where each variable name (including the intercept if `create_intercept` is `True`) is the key, and the value is another dictionary, with keys for `coefficient`, `standard_error`, `z_stat`, and `p_value`. The coefficient should be the log odds-ratio (which takes the place of the coefficients in OLS)\n",
        "- a method called `fit_model` that uses the `self.regression_type` attribute to determine whether or not to run an OLS or Logistic Regression using the data provided. This method should call the correct regression method.\n",
        "- an updated method (call it `summary`) that presents your regression results in a table\n",
        "    - Columns should be: Variable name, Log odds-ratio value, standard error, z-statistic, and p-value, in that order.\n",
        "    - Your summary table should have different column titles for OLS and Logistic Regressions! (think if statement...)\n",
        "\n",
        "You only need to define the class. My code will create an instance of your class (be sure all the names match these instructions, and those from last week!), and provide data to run a regression. I will provide the same data to you, so that you can experiment and make sure that your code is functioning properly.\n",
        "\n",
        "NOTE: I have created a [primer on Logistic regression](https://github.com/dustywhite7/Econ8320/blob/master/SlidesPDF/9-2%20-%20Logit%20Primer.pdf) to go with this assignment. See the Github slidesPDF folder\n",
        "\n",
        "Put the code that you would like graded in the cell labeled `#si-exercise`. I recommend copying your code from the last assignment (in chapter 9 about linear regression and `numpy`), and continuing from there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "6a146533",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a146533",
        "outputId": "d241f62f-26a9-441d-a941-b887ba4ead34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:72: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "<>:72: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "<ipython-input-60-ad3411e4c4b2>:72: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "  predicitions = 1/ 1(1 + np.exp(-linear_combination))\n"
          ]
        }
      ],
      "source": [
        "#si-exercise\n",
        "\n",
        "#regression model from last class\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "\n",
        "class RegressionModel:\n",
        "    def __init__(self, x, y, create_intercept=True, regression_type='ols'):\n",
        "        self.x = x.copy()\n",
        "        self.y = y\n",
        "        self.create_intercept = create_intercept\n",
        "        self.regression_type = regression_type\n",
        "        self.results = {}\n",
        "        if self.create_intercept:\n",
        "            self.add_intercept()\n",
        "\n",
        "    def add_intercept(self):\n",
        "\n",
        "        self.x['intercept'] = 1\n",
        "\n",
        "    def ols_regression(self):\n",
        "\n",
        "        X = self.x.values\n",
        "        Y = self.y.values.reshape(-1, 1)\n",
        "\n",
        "        XtX_inv = np.linalg.inv(np.dot(X.T, X))\n",
        "        XtY = np.dot(X.T, Y)\n",
        "        coefficients = np.dot(XtX_inv, XtY)\n",
        "\n",
        "        predictions = np.dot(X, coefficients)\n",
        "        residuals = Y - predictions\n",
        "\n",
        "        n, k = X.shape\n",
        "        s_square = (residuals.T @ residuals) / (n - k)\n",
        "        variance = s_square[0][0]\n",
        "        standard_errors = np.sqrt(np.diag(variance * XtX_inv))\n",
        "\n",
        "\n",
        "        t_stats = coefficients.flatten() / standard_errors\n",
        "        p_values = (1 - t.cdf((t_stats), df=n - k))\n",
        "\n",
        "\n",
        "        for i, col in enumerate(self.x.columns):\n",
        "            self.results[col] = {\n",
        "                'coefficient': coefficients[i][0],\n",
        "                'standard_error': standard_errors[i],\n",
        "                't_stat': t_stats[i],\n",
        "                'p_value': p_values[i]\n",
        "            }\n",
        "    def logistic_regression(self):\n",
        "      def loglike(b,y,x):\n",
        "        top = np.exp(np.dot(x,b))\n",
        "        bottom = 1  + top\n",
        "        gamma = top/bottom\n",
        "        loglike = -1*np.sum(y*np.log(gamma) + (1-y)*np.log(1-gamma))\n",
        "        return loglike\n",
        "\n",
        "      #def grad_loglike(b,y,x):  #something about the derrivative, im getting stuck here ughgbghbkmf,d\n",
        "       # top = np.exp(np.dot(x,b))\n",
        "        #bottom = 1  + top\n",
        "        #gamma = top/bottom\n",
        "       # grad\n",
        "\n",
        "       #retry, what other classmate said helped\n",
        "      #def grad_loglike(b):  #something about the derrivative, im getting stuck here ughgbghbkmf,d\n",
        "       # top = np.dot(x, b)\n",
        "       # X_t_b = 1/ #x times beta\n",
        "\n",
        "        def grad_loglike(beta):\n",
        "          linear_combination = np.dot(self.x.values, beta)\n",
        "          predicitions = 1/ 1(1 + np.exp(-linear_combination))\n",
        "          return -np.dot(self.x.T.values, self.y - predictions)\n",
        "\n",
        "        initial_beta = np.zeroes(self.x.shape[1])\n",
        "        result = minimize(loglike, initial_beta, args=(self.y, self.x))\n",
        "\n",
        "        if not result.success:\n",
        "          raise RuntimeError(\"Optimization Failed\")\n",
        "\n",
        "        coefficient = result.x\n",
        "        self.results\n",
        "\n",
        "        predictions = 1/(1 + np.exp(-np.dot(self.x.values, beta_hat)))\n",
        "\n",
        "        n,k = self.x.shape\n",
        "        hessian_matrix = np.zeroes((k,k))\n",
        "\n",
        "        for i in range(n):\n",
        "          predi_i = predictions[i]\n",
        "          hessian_matrix += predi_i * (1 - predi_i) * np.outer(self.x.iloc[i].values, self.x.iloc[i].values)\n",
        "\n",
        "        hessian_matrix += np.eye(k) * 1e6\n",
        "\n",
        "        try:\n",
        "          inverse_hessian = np.linalg.inv(hessian_matrix)\n",
        "          standard_error = np.sqrt(np.diag(inverse_hessian))\n",
        "        except np.linalg.LinAlgError:\n",
        "\n",
        "          standard_error = np.full(k, np.na )\n",
        "\n",
        "          z_stat = beta_hat / standard_error\n",
        "          p_value = 1 - norm.cdf(np.abs(z_stat))      #2 * (1 - t.cdf(np.abs(t_stats), df=n - k))\n",
        "\n",
        "\n",
        "        for i, col in enumerate(self.x.columns):\n",
        "            self.results[col] = {\n",
        "                'coefficient': coefficient[i][0],\n",
        "                'standard_error': standard_error[i],\n",
        "                'z_stat': z_statz[i],\n",
        "                'p_value': p_value[i]\n",
        "            }\n",
        "\n",
        "    def fit_model(self):\n",
        "        if self.regression_type == 'ols':\n",
        "            self.ols_regression()\n",
        "        elif self.regression_type == 'logit':\n",
        "            self.logistic_regression()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid regression type\")\n",
        "\n",
        "    def summary(self):\n",
        "        self.ols_regression()\n",
        "        if self.regression_type == 'ols':\n",
        "           summary_df = pd.DataFrame({\n",
        "            \"Variable name\": list(self.results.keys()),\n",
        "            \"coefficient value\": [self.results[var]['coefficient'] for var in self.results],\n",
        "            \"standard error\": [self.results[var]['standard_error'] for var in self.results],\n",
        "            \"t-statistic\": [self.results[var]['t_stat'] for var in self.results],\n",
        "            \"p-value\": [self.results[var]['p_value'] for var in self.results]\n",
        "        })\n",
        "        elif self.regression_type == 'logit':\n",
        "           summary_df = pd.DataFrame({\n",
        "            \"Variable name\": list(self.results.keys()),\n",
        "            \"coefficient value\": [self.results[var]['coefficient'] for var in self.results],\n",
        "            \"standard error\": [self.results[var]['standard_error'] for var in self.results],\n",
        "            \"t-statistic\": [self.results[var]['t_stat'] for var in self.results],\n",
        "            \"p-value\": [self.results[var]['p_value'] for var in self.results]\n",
        "        })\n",
        "\n",
        "        print(summary_df)\n",
        "        return summary_df\n",
        "\n",
        "#end of regression model from last class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing code\n",
        "data = pd.read_csv(\"/content/assignment8Data.csv\")\n",
        "x = data.loc[:100, ['sex','age','educ']]\n",
        "y = data.loc[:100, 'white']\n",
        "reg = RegressionModel(x, y, create_intercept=True, regression_type='logit')\n",
        "reg.fit_model()\n",
        "\n",
        "sex = {'coefficient': -1.1229156890097627,\n",
        "      'standard_error': 0.39798772782618025,\n",
        "      'z_stat': -2.821483202869492,\n",
        "      'p_value': 0.004780214077269219}\n",
        "age = {'coefficient': -0.007012518056833769,\n",
        "      'standard_error': 0.010835821823286998,\n",
        "      'z_stat': -0.6471607019011091,\n",
        "      'p_value': 0.5175279421902776}\n",
        "educ = {'coefficient': -0.046485475816343394,\n",
        "      'standard_error': 0.10100278092776117,\n",
        "      'z_stat': -0.46023956359766527,\n",
        "      'p_value': 0.6453442758780246}\n",
        "intercept = {'coefficient': 5.735435005488546,\n",
        "      'standard_error': 1.1266207023561843,\n",
        "      'z_stat': 5.090830475148922,\n",
        "      'p_value': 3.56498650369634e-07}\n",
        "\n",
        "sexEq = (round(sex['coefficient']-reg.results['sex']['coefficient'], 1)==0) & (round(sex['standard_error']-reg.results['sex']['standard_error'], 1)==0) & (round(sex['z_stat']-reg.results['sex']['z_stat'], 1)==0) & (round(sex['p_value']-reg.results['sex']['p_value'], 1)==0)\n",
        "ageEq = (round(age['coefficient']-reg.results['age']['coefficient'], 1)==0) &  (round(age['standard_error']-reg.results['age']['standard_error'], 1)==0) & (round(age['z_stat']-reg.results['age']['z_stat'], 1)==0) & (round(age['p_value']-reg.results['age']['p_value'], 1)==0)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "0JTMNJYw55t_",
        "outputId": "2016c27a-2b87-46fb-a8e5-3eb94dbff56e"
      },
      "id": "0JTMNJYw55t_",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sex'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-d79a4ac9de75>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       'p_value': 3.56498650369634e-07}\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msexEq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coefficient'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coefficient'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_stat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_stat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mageEq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coefficient'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coefficient'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_stat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_stat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sex'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other stuff for reference below:"
      ],
      "metadata": {
        "id": "iLM5UOynQCyr"
      },
      "id": "iLM5UOynQCyr"
    },
    {
      "cell_type": "code",
      "source": [
        "#class video example for lesson 10\n",
        "\n",
        "\n",
        "#estimate a gradient of the following function\n",
        "#y = sin(x) - (z+3)**@\n",
        "import numpy as np\n",
        "\n",
        "def y(x, z):\n",
        "  return np.sin(x) - (z+3)**2\n",
        "\n",
        "y(1,10) #this gives us the value of y at x =1 and z =10\n",
        "\n",
        "y(-3,2)\n",
        "\n",
        "#make a gradient for the slope in each direction, find different points for different slopes\n",
        "def grad_y(x,z):\n",
        "  slope_x = (y(x + 0.05, z) - y(x,z))/0.05  #find the derrivative in thex direction (this is the slope in the x direction rise/run)\n",
        "  slope_z = (y(x, z + 0.05) - y(x,z))/0.05 #find the derrivative in the z direction (this is the slope in the z direction rise/run)\n",
        "  return (slope_x, slope_z)\n",
        "\n",
        "grad_y(1,10) #shows the slope in the x and z position\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVVVCA1usJ_q",
        "outputId": "ca1cc1da-be3d-4e3b-aaee-0c469acc5c40"
      },
      "id": "RVVVCA1usJ_q",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5190448157225092, -26.050000000000182)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in class example continued\n",
        "\n",
        "#minimization examples (there is no functon for maxamizing because it is the same as minimizing just mulitply by -1)\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "#need to flip our function since its concave in order to turn it from max to min, minimize needs to be able to take a list of values( an array) snd not just 2 values. So our function just needs to be able to take one input of an array of xs\n",
        "\n",
        "def q(xs):\n",
        "  return -1*y(xs[0], xs[1])\n",
        "\n",
        "res = minimize(q, [0,0]) #q is the function we want to pass and [0,0] is starting point\n",
        "res\n",
        "\n",
        "#this finds that our optomum is for x = 1.571 and z = -3\n",
        "\n",
        "#plug these back into y function from above\n",
        "\n",
        "y(1.571, -3) #what we get back from this is essentially 1\n",
        "\n",
        "q([1.57, -3]) #is negative 1 because it is flipped of y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2-idPg8yaqn",
        "outputId": "1c3e1f41-b715-4984-ae6f-0b72a5ed36f3"
      },
      "id": "d2-idPg8yaqn",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9999996829318346"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try with different starting point at 10,10\n",
        "minimize(q, [10,10])\n",
        "\n",
        "#find that the optimum x changes (7.854) but the z stays the same(-3), this is because we have a sin function in the def y fucntion we defnied above. We have many minimum values wehre we have the same z but depending on where we start we can get a different optimal x\n",
        "#because of this we cant randomly choose a starting point becuase this could change the optmum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-NUjVrdzcZv",
        "outputId": "be0eb8b4-27a2-4415-9f87-a036bf7973c3"
      },
      "id": "l-NUjVrdzcZv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  message: Optimization terminated successfully.\n",
              "  success: True\n",
              "   status: 0\n",
              "      fun: -0.9999999999992139\n",
              "        x: [ 7.854e+00 -3.000e+00]\n",
              "      nit: 6\n",
              "      jac: [ 1.863e-07 -1.743e-06]\n",
              " hess_inv: [[ 1.000e+00 -1.399e-03]\n",
              "            [-1.399e-03  5.070e-01]]\n",
              "     nfev: 24\n",
              "     njev: 8"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression (check this video for the function ill have to use for to solve this question on homework)\n",
        "# output is now the probability of success -> p(y=1)\n",
        "\n",
        "#any number raised to negative infinity is 0\n",
        "\n",
        "sol = minimize(q, [0,0])\n",
        "sol.x #<- the paramater attatched here are the beta coefficients for our model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6sF02qd1l3T",
        "outputId": "90ea65aa-b108-4a86-8a7c-1a4b9875f89c"
      },
      "id": "b6sF02qd1l3T",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.57079632, -3.00000001])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLS MLE FUNCTION EXAMPLE"
      ],
      "metadata": {
        "id": "-x5iDWEMKRgI"
      },
      "id": "-x5iDWEMKRgI"
    },
    {
      "cell_type": "code",
      "source": [
        "#OLS MLE Function (building out likelihood function)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#betas are k long\n",
        "#sigma^2 is our variance and a single number\n",
        "#pass in a single array and then extract the last value to be sigma^2\n",
        "\n",
        "def ols_mle(theta, x, y): #theta is our general parameters (out inputs so our betas + our sigmas)\n",
        "  beta = theta[:-1] #everything except for the last theta\n",
        "  sig2 = theta[-1] #our last theta is going to be our sigma squared, all other thetas will be betas as defined above\n",
        "  n = x.shape[0] #this is the number of rows in x\n",
        "\n",
        "  #generate our likelihood\n",
        "  first = -1 * (n/2) *np.log(2*np.pi) #first part of our likelihood function\n",
        "  second = -1 * (n/2) *np.log(sig2)\n",
        "  epsilon = y - (x @ beta) #this is our error #y, x, and beta are matrixes in this equation so we have to input them here with matrix algebra\n",
        "  third = -1 * ((1/2*sig2)) * (epsilon.T @ epsilon) #epsilon.T trnasnposes\n",
        "\n",
        "  return -1 * (first + second + third) #the negative one helps makes this minimzie (because orignially this function would find the max)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sq6k50pN4a_X"
      },
      "id": "Sq6k50pN4a_X",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "minimize(ols_mle[0,0,0], #pass in a and y if not doing with a class) #if we had 2 x's we need 3 rwos because this will pass through x1, x2, and signma squared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "pyn37E_7FXq5",
        "outputId": "125409de-c899-4ddf-b899-ec6862fbb770"
      },
      "id": "pyn37E_7FXq5",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'function' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8812e004ad47>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mols_mle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if we had 2 x's we need 3 rwos because this will pass through x1, x2, and signma squared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "END MLE FUNCTION EXAMPLE"
      ],
      "metadata": {
        "id": "Whzjr2rfKYKj"
      },
      "id": "Whzjr2rfKYKj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "OTHER STATS HELP From walkthrough code\n"
      ],
      "metadata": {
        "id": "7-WRWwilKdWJ"
      },
      "id": "7-WRWwilKdWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#dot product\n",
        "a = [1,2,3,4]\n",
        "b = [10,20,31,3]\n",
        "def dot_prod(a,b):\n",
        "  dot = 0\n",
        "  for i in range(len(a)):\n",
        "    dot += a[i] * b[i]\n",
        "  return dot\n",
        "\n",
        "dot_prod(a,b)\n",
        "\n",
        "#or we can do a second way\n",
        "\n",
        "def dot_prod(a,b):\n",
        "  return sum([i*j for i,j in zip(a,b)])\n",
        "\n",
        "dot_prod(a,b)\n",
        "\n",
        "\n",
        "#Matrix multiplication, we wont use but was just an example from videos\n",
        "\n",
        "def mat_mul(A,B):\n",
        "  matrix = []\n",
        "  for i in range(len(A)):\n",
        "    m_row = []\n",
        "    for j in range(len(B[0])):\n",
        "      row = A[i]\n",
        "      col = [b_row[i] for b_row in B]\n",
        "      m_row.append(dot_prod(row, col))\n",
        "    matrix.append(m_row)\n",
        "  return matrix\n",
        "\n",
        "\n",
        "A = [[1,2],[3,4]]\n",
        "B =[[6], [5]]\n",
        "\n",
        "mat_mul(A,B)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "H4n2-7ZYKmn4",
        "outputId": "e5f2ed39-ec89-49d3-d9dc-8c802c5eb169"
      },
      "id": "H4n2-7ZYKmn4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 45 (<ipython-input-19-6232cc7b2d20>, line 46)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-6232cc7b2d20>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    if len(b[i]) is not len(b[0]): # If not,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the code directl from the slides  for matrix multiplication that actual worked\n",
        "\n",
        "def matMul(a, b): # Define function, take 2 matrices\n",
        "  for i in range(len(a)): # Make sure a is matrix\n",
        "    if len(a[i]) is not len(a[0]): # If not,\n",
        "      raise RuntimeError( # Raise an error\n",
        "        \"Matrix A is not correctly specified\")\n",
        "  for i in range(len(b)): # Make sure b is a matrix\n",
        "    if len(b[i]) is not len(b[0]): # If not,\n",
        "      raise RuntimeError( # Raise an error\n",
        "        \"Matrix B is not correctly specified\")\n",
        "  matrix = [] # Initialize new empty matrix\n",
        "  if len(a[0]) is len(b): # Test for conformability\n",
        "    for i in range(len(a)): # Iterate over rows of a\n",
        "      row = [] # Create row of new matrix\n",
        "      for j in range(len(b[0])): # Iterate over columns\n",
        "        row.append(dot_prod(a[i], # Append elements of col\n",
        "          [b[n][j] for n in range(len(b))]))\n",
        "        matrix.append(row) # Append rows to matrix\n",
        "    return matrix # Return the newly calculated matrix\n",
        "  else: # If matrices are nonconforming\n",
        "    raise RuntimeError( #Raise an error\n",
        "      \"Matrices are nonconformable for multiplication\")\n",
        "\n",
        "A = [[1,2],[3,4]]\n",
        "B =[[6], [5]]\n",
        "\n",
        "matMul(A,B)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH33FSCcPquX",
        "outputId": "251ba4c7-cc77-469a-8760-70866f30d706"
      },
      "id": "EH33FSCcPquX",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[16], [38]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import T\n",
        "#using numpy instead for matrix multiplication\n",
        "\n",
        "#core element of numpy is arrays\n",
        "import numpy as np\n",
        "\n",
        "A = [[1,2],[3,4]]\n",
        "B =[[6], [5]]\n",
        "\n",
        "np.array(A)\n",
        "\n",
        "A = np.array(A)\n",
        "np.shape(A) #tells me that a is a 2x2 matrix\n",
        "\n",
        "A.T #this will give me the A matrix transposed\n",
        "\n",
        "B = np.array(B)\n",
        "np.shape(B)\n",
        "\n",
        "A @ B #this @ will mutiply out matrixs'\n",
        "\n",
        "np.zeros((3,3)) #gives us a 3x3 matrix with zeros in it, can create a tensor using (3,3,3)\n",
        "\n",
        "np.eye(3) #gives us a 3x3 identity matrix\n",
        "\n",
        "A.reshape(4,1) #this will reshape matrix a to be 4x1\n",
        "\n",
        "\n",
        "A + 1 #this will add 1 to every element in A matrix (can do the same with subtraction and multiplication)\n",
        "\n",
        "A.T.dot(A) #this will give us A**2 (A matrix * A matrix) had to transpose the first a first to get this before multplication using the dot function\n",
        "\n",
        "A.T @ A #this will also let us square the matrix\n",
        "\n",
        "\n",
        "\n",
        "#in class excersie\n",
        "\n",
        "# y = a + b * x + c * x**2, try to create a function in these terms using matrix multiplication\n",
        "\n",
        "def calc_y(a,b,c,x):\n",
        "  mat1 = np.array([[a,b,c]]) #have to make sure everything is an array and not just a regular list with just []\n",
        "  mat2 = np.array([[1, x, x**2]])\n",
        "  y = mat1 @ mat2.T\n",
        "  return y\n",
        "\n",
        "calc_y(1,1,1,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj0-7r7SXljO",
        "outputId": "e1a76e2f-a2bd-4683-a2fc-641040d9a73a"
      },
      "id": "Aj0-7r7SXljO",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#random numbers\n",
        "np.random.rand()\n",
        "np.random.rand(10,1) #give be a 10x1 array of random numbers\n",
        "\n",
        "#example: generate a sample of 10 obs from Exponenetial Distribution, where lambda = 1 (Look up CDF of exponential Distribution, and solve for x)\n",
        "\n",
        "\n",
        "p = 1 - exp(-1 * x) #for cdf lambda = 1 so the values in teh parenthesis just become -x\n",
        "\n",
        "#if were drawing p we need to invert this so that we can calculate x\n",
        "\n",
        "exp (-1 * x) = 1-p\n",
        "#get rid of exponentiation by taking the log\n",
        "-x = np.log(1-p)\n",
        "#now invert this\n",
        "x = -1 * np.log(1-p)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "wbTs9Sa1cOSL",
        "outputId": "e8c4609f-7187-4442-e184-3e45ce6c568e"
      },
      "id": "wbTs9Sa1cOSL",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (<ipython-input-52-ddfc4f35556c>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-ddfc4f35556c>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    exp (-1 * x) = 1-p\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make a function using what we learned above, this will give us values from the exponential distribution\n",
        "def exp():\n",
        "  return -1 * np.log(1-np.random.rand())\n",
        "\n",
        "exp()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7vnmcB2fHyD",
        "outputId": "fc3e9772-5500-4ec8-bd92-b863df684ea4"
      },
      "id": "s7vnmcB2fHyD",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.17229774, 0.54666556, 1.75785322, 1.14429244, 0.68458634,\n",
              "       0.18428722, 0.95002375, 0.69869035, 1.17329203, 0.13358234])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get it where with n argument ir will give us n numbers from the exponential distribtion\n",
        "def exp(n):\n",
        "  return -1 * np.log(1-np.random.rand(n))\n",
        "\n",
        "exp(10)"
      ],
      "metadata": {
        "id": "yIPmh-SpffA6"
      },
      "id": "yIPmh-SpffA6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribututional calculations\n",
        "import scipy.stats as ss\n",
        "\n",
        "ss.norm.sf(2) #will give us probability of a value being above the z score (in this case we did z score 2) will be (2.27%)\n",
        "\n",
        "#allows us to do critical value and waht not\n",
        "\n",
        "# to see probabilty below we use cdf\n",
        "ss.norm.cdf(3) #shows at z score of the 3 the probabilty of a value being below z score (3) is .998 or 99.8%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Uww7Fgfg_s",
        "outputId": "9f1363a8-7cfe-4750-eaca-f866362ea91c"
      },
      "id": "h8Uww7Fgfg_s",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9986501019683699"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}